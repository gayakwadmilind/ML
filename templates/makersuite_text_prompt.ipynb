{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tce3stUlHN0L"
      },
      "source": [
        "##### Copyright 2023 Google LLC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "tuOe1ymfHZPu"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kWIuwKG2_oWE"
      },
      "outputs": [],
      "source": [
        "# Install the client library and import necessary modules.\n",
        "!pip install google-generativeai\n",
        "import google.generativeai as palm\n",
        "import base64\n",
        "import json\n",
        "import pprint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_SvYoR3WCeKr"
      },
      "outputs": [],
      "source": [
        "# Configure the client library by providing your API key.\n",
        "palm.configure(api_key=\"YOUR API KEY\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pwab0i0RAHBN"
      },
      "outputs": [],
      "source": [
        "# These parameters for the model call can be set by URL parameters.\n",
        "model = 'models/text-bison-001' # @param {isTemplate: true}\n",
        "temperature = 0.7 # @param {isTemplate: true}\n",
        "candidate_count = 1 # @param {isTemplate: true}\n",
        "top_k = 40 # @param {isTemplate: true}\n",
        "top_p = 0.95 # @param {isTemplate: true}\n",
        "max_output_tokens = 1024 # @param {isTemplate: true}\n",
        "text_b64 = 'SSBsaWtlIHRvIHNsZWVwIDggaG91cnMgZGFpbHkuIEkgaGF2ZSBkaW5uZXIgYXQgMTAgcC5tLiBkYWlseS4gSSBoYXZlIGJyZWFrZmFzdCBkYWlseSBpbiB0aGUgbW9ybmluZy4gSSB0YWtlIDE1IG1pbnV0ZXMgZm9yIHRoZSBicmVha2Zhc3QuIFlvdSB3YW50IHRvIHN0YXJ0IHlvdXIgcHJlc2VudGF0aW9uIGF0IDEwOjMwIGEubS4gdG9kYXkuIEl0IHRha2VzIDMwIG1pbnV0ZXMgdG8gcmVhY2ggdG8gY29sbGVnZSBmcm9tIHlvdXIgaG9tZS4gV2hlbiBzaG91bGQgeW91IGxlYXZlIHRoZSBob21lIGZvciBjb2xsZWdlIHNvIGFzIHRvIHJlYWNoIDEwIG1pbnV0ZXMgcHJpb3IgdG8gdGhlIHNjaGVkdWxlZCBwcmVzZW50YXRpb24gdGltZSwgYXNzdW1pbmcgeW91IHdha2UgdXAgYXQgODowMCBhLm0uPw==' # @param {isTemplate: true}\n",
        "stop_sequences_b64 = 'W10=' # @param {isTemplate: true}\n",
        "safety_settings_b64 = 'W3siY2F0ZWdvcnkiOiJIQVJNX0NBVEVHT1JZX0RFUk9HQVRPUlkiLCJ0aHJlc2hvbGQiOjF9LHsiY2F0ZWdvcnkiOiJIQVJNX0NBVEVHT1JZX1RPWElDSVRZIiwidGhyZXNob2xkIjoxfSx7ImNhdGVnb3J5IjoiSEFSTV9DQVRFR09SWV9WSU9MRU5DRSIsInRocmVzaG9sZCI6Mn0seyJjYXRlZ29yeSI6IkhBUk1fQ0FURUdPUllfU0VYVUFMIiwidGhyZXNob2xkIjoyfSx7ImNhdGVnb3J5IjoiSEFSTV9DQVRFR09SWV9NRURJQ0FMIiwidGhyZXNob2xkIjoyfSx7ImNhdGVnb3J5IjoiSEFSTV9DQVRFR09SWV9EQU5HRVJPVVMiLCJ0aHJlc2hvbGQiOjJ9XQ==' # @param {isTemplate: true}\n",
        "\n",
        "# Convert the prompt text param from a bae64 string to a string.\n",
        "text = base64.b64decode(text_b64).decode(\"utf-8\")\n",
        "\n",
        "# Convert the stop_sequences and safety_settings params from base64 strings to lists.\n",
        "stop_sequences = json.loads(base64.b64decode(stop_sequences_b64).decode(\"utf-8\"))\n",
        "safety_settings = json.loads(base64.b64decode(safety_settings_b64).decode(\"utf-8\"))\n",
        "\n",
        "defaults = {\n",
        "  'model': model,\n",
        "  'temperature': temperature,\n",
        "  'candidate_count': candidate_count,\n",
        "  'top_k': top_k,\n",
        "  'top_p': top_p,\n",
        "  'max_output_tokens': max_output_tokens,\n",
        "  'stop_sequences': stop_sequences,\n",
        "  'safety_settings': safety_settings,\n",
        "}\n",
        "\n",
        "# Show what will be sent with the API call.\n",
        "pprint.pprint(defaults | {'prompt': text})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LB2LxPmAB95V"
      },
      "outputs": [],
      "source": [
        "# Call the model and print the response.\n",
        "response = palm.generate_text(\n",
        "  **defaults,\n",
        "  prompt=text\n",
        ")\n",
        "print(response.candidates[0]['output'])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "makersuite_text_prompt.ipynb",
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}